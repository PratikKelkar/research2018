{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from heapq import heappush, heappop, heappushpop\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the presaved channel data and word/category data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.load(\"all_data.npy\") #holds all the data from channels\n",
    "category_info = np.load(\"words_in_categories.npy\") #category_info[cat][ptr] returns the number of the word(0...62) of the ptr'th word in the category cat\n",
    "lengths = np.load(\"category_lengths.npy\") #lengths[cat] is the number of words in category cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = 63 \n",
    "\n",
    "tStart = 0 #start time\n",
    "tEnd = 650 #end time\n",
    "tWidth = 100 #width of time slice\n",
    "tIncr = 50 #increment in start time\n",
    "tEx = 10 #number of examples to downsample to\n",
    "\n",
    "training_amt = 8 #8 examples for training, 2 for testing\n",
    "testing_amt = 10 - training_amt\n",
    "\n",
    "np.random.seed(63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and build TrainingData and TestingData matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData = np.zeros((total_words,5,training_amt,256,650))#gives the pertinent data from all_data for the two categories\n",
    "TestingData = np.zeros( (total_words,5,testing_amt,256,650)) #^\n",
    "wordptr = -1 #the index of the current word, iterates from 0...total_words\n",
    "\n",
    "for i in range(63):\n",
    "    wordptr+=1\n",
    "\n",
    "    excl = [-1]*10 #excl[j] = the j'th presentation number which should be saved for testing (e.g. excl[0] = 0 means the first presentation of the wordptr'th word should be saved for testing). Ignore -1's.\n",
    "    \n",
    "    for pres in range(testing_amt):\n",
    "        while(1): #this loop repeatedly generates a random presentation until one which hasn't been reserved for testing has been found, and then breaks it\n",
    "            nxtrand = np.random.randint(0,10)\n",
    "            if(excl[nxtrand]==-1):\n",
    "                excl[nxtrand]=nxtrand\n",
    "                break\n",
    "    for bandnum in range(5):\n",
    "        ptr2 = 0 #points to which presentation(0...9) of wordptr'th word we are currently copying to TrainingData\n",
    "        for pres in range(10):\n",
    "            if(excl[pres]!=-1): #if reserved for testing, don't include in training data\n",
    "                continue\n",
    "           \n",
    "            TrainingData[wordptr][bandnum][ptr2]=all_data[bandnum][i][pres] #sets the channel x time matrix for TrainingData[bandnum][wordptr][ptr2]\n",
    "            ptr2+=1 #move to next presentation\n",
    "\n",
    "    for bandnum in range(5): #this loop is same as above, except now we only want the testing presentations\n",
    "        ptr2=0\n",
    "        for pres in range(10):\n",
    "            if(excl[pres]==-1):\n",
    "                continue\n",
    "            TestingData[wordptr][bandnum][ptr2] = all_data[bandnum][i][excl[pres]]\n",
    "            ptr2+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vectors to hold best feature information based on Pearson Coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toSelect = 5 #number of top features to select\n",
    "\n",
    "best_feature_vectors = np.zeros((total_words, training_amt,toSelect * tEx))\n",
    "test_feature_vectors = np.zeros((total_words, testing_amt, toSelect * tEx))\n",
    "timeSequences = np.zeros((total_words,5,12,training_amt,256,tEx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick top feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 5, 12, 8, 256, 10)\n",
      "Word 0\n",
      "5. 0   500   58   0.97414626929\n",
      "4. 0   250   16   0.976603672895\n",
      "3. 0   250   29   0.982799887577\n",
      "2. 0   250   23   0.984346319517\n",
      "1. 0   450   39   0.986853579328\n",
      "Word 1\n",
      "5. 0   350   247   0.922771480833\n",
      "4. 0   450   31   0.927248448648\n",
      "3. 0   400   247   0.936575436008\n",
      "2. 0   300   73   0.947596205345\n",
      "1. 0   200   239   0.950823540383\n",
      "Word 2\n",
      "5. 0   150   233   0.931991434803\n",
      "4. 0   450   58   0.938864194273\n",
      "3. 0   450   144   0.939616281831\n",
      "2. 0   450   243   0.940000287145\n",
      "1. 0   450   145   0.962295117817\n",
      "Word 3\n",
      "5. 0   200   38   0.984405804446\n",
      "4. 0   200   46   0.984589580238\n",
      "3. 0   200   28   0.986547647042\n",
      "2. 0   200   29   0.987102458171\n",
      "1. 0   400   35   0.989467676496\n",
      "Word 4\n",
      "5. 0   150   69   0.976214667503\n",
      "4. 0   300   57   0.977821228133\n",
      "3. 0   150   56   0.978149087461\n",
      "2. 0   150   47   0.986665314791\n",
      "1. 0   150   239   0.994502272076\n",
      "Word 5\n",
      "5. 0   300   35   0.960012413972\n",
      "4. 0   100   190   0.962055774329\n",
      "3. 0   100   171   0.968798208269\n",
      "2. 0   100   208   0.97150707801\n",
      "1. 0   100   162   0.977770667175\n",
      "Word 6\n",
      "5. 0   550   82   0.943766414604\n",
      "4. 0   200   50   0.950625410629\n",
      "3. 0   550   83   0.950633840697\n",
      "2. 0   400   131   0.979545094769\n",
      "1. 0   400   8   0.984497616411\n",
      "Word 7\n",
      "5. 0   200   55   0.940071503831\n",
      "4. 0   200   56   0.944508405207\n",
      "3. 0   200   23   0.949203381149\n",
      "2. 0   200   49   0.954855990403\n",
      "1. 0   200   48   0.979355900582\n",
      "Word 8\n",
      "5. 0   200   9   0.980728093778\n",
      "4. 0   150   1   0.981493813907\n",
      "3. 0   150   49   0.984404599918\n",
      "2. 0   0   1   0.986656705995\n",
      "1. 0   0   2   0.989283851767\n",
      "Word 9\n",
      "5. 0   250   238   0.993268970524\n",
      "4. 0   250   29   0.995056477665\n",
      "3. 0   200   35   0.995640566053\n",
      "2. 0   250   35   0.995965880448\n",
      "1. 0   200   29   0.997257101213\n",
      "Word 10\n",
      "5. 0   400   12   0.987283659872\n",
      "4. 0   450   120   0.987714767273\n",
      "3. 0   200   22   0.990494153352\n",
      "2. 0   450   104   0.991724555661\n",
      "1. 0   400   27   0.99328277459\n",
      "Word 11\n",
      "5. 0   100   109   0.912105983376\n",
      "4. 0   150   149   0.917123185311\n",
      "3. 0   100   99   0.924724886536\n",
      "2. 0   100   79   0.929053804364\n",
      "1. 0   100   89   0.938166270256\n",
      "Word 12\n",
      "5. 1   100   166   0.880478300317\n",
      "4. 0   450   204   0.886213719636\n",
      "3. 1   100   174   0.890036756731\n",
      "2. 1   100   158   0.901135749358\n",
      "1. 0   500   221   0.919908073993\n",
      "Word 13\n",
      "5. 0   350   200   0.993131649319\n",
      "4. 0   350   177   0.994763755387\n",
      "3. 0   350   152   0.994769735704\n",
      "2. 0   350   190   0.994814792101\n",
      "1. 0   350   178   0.995075884567\n",
      "Word 14\n",
      "5. 0   0   69   0.921004363531\n",
      "4. 0   450   48   0.926288786273\n",
      "3. 0   450   20   0.948476291726\n",
      "2. 0   450   35   0.953255728631\n",
      "1. 0   500   35   0.953478834012\n",
      "Word 15\n",
      "5. 0   450   21   0.886791386712\n",
      "4. 0   150   249   0.901387376779\n",
      "3. 0   150   49   0.902202540366\n",
      "2. 0   450   28   0.912400047434\n",
      "1. 0   200   249   0.988277580139\n",
      "Word 16\n",
      "5. 0   150   233   0.927244943136\n",
      "4. 0   150   49   0.934085551506\n",
      "3. 0   0   50   0.949205756666\n",
      "2. 0   150   9   0.97229543648\n",
      "1. 0   200   9   0.98085190589\n",
      "Word 17\n",
      "5. 0   200   35   0.966648531421\n",
      "4. 0   450   238   0.967391013249\n",
      "3. 0   200   38   0.968127894532\n",
      "2. 0   400   40   0.969969449908\n",
      "1. 0   250   239   0.9916586231\n",
      "Word 18\n",
      "5. 0   200   51   0.971247525669\n",
      "4. 0   150   40   0.973366558256\n",
      "3. 0   250   80   0.979462353788\n",
      "2. 0   0   41   0.979577536901\n",
      "1. 0   200   9   0.984609379155\n",
      "Word 19\n",
      "5. 0   150   49   0.976253961982\n",
      "4. 0   150   34   0.978157302035\n",
      "3. 0   150   27   0.981039338162\n",
      "2. 0   200   50   0.981613163177\n",
      "1. 0   350   27   0.986139597001\n",
      "Word 20\n",
      "5. 0   350   20   0.965475294739\n",
      "4. 0   350   38   0.970029833645\n",
      "3. 0   400   45   0.971934115665\n",
      "2. 0   450   45   0.972063077435\n",
      "1. 0   350   33   0.974226343769\n",
      "Word 21\n",
      "5. 0   100   167   0.981945678949\n",
      "4. 0   400   46   0.98399006477\n",
      "3. 0   300   252   0.984676980372\n",
      "2. 0   350   247   0.989489704082\n",
      "1. 0   300   247   0.996384138456\n",
      "Word 22\n",
      "5. 0   350   49   0.958991942448\n",
      "4. 0   0   234   0.960709161778\n",
      "3. 0   150   208   0.960943276413\n",
      "2. 0   150   39   0.970018787348\n",
      "1. 0   0   230   0.971063119989\n",
      "Word 23\n",
      "5. 0   350   253   0.951775118009\n",
      "4. 0   350   166   0.952662438676\n",
      "3. 0   350   150   0.95846922549\n",
      "2. 0   350   157   0.966296507514\n",
      "1. 0   350   148   0.973474267016\n",
      "Word 24\n",
      "5. 0   200   12   0.988336611663\n",
      "4. 0   200   19   0.989901363148\n",
      "3. 0   250   19   0.99406692877\n",
      "2. 0   250   18   0.994271484222\n",
      "1. 0   200   18   0.994890819187\n",
      "Word 25\n",
      "5. 0   250   6   0.966172360809\n",
      "4. 0   200   15   0.966234077123\n",
      "3. 0   200   6   0.96827691234\n",
      "2. 0   300   206   0.97310714716\n",
      "1. 0   250   28   0.979780866645\n",
      "Word 26\n",
      "5. 0   100   163   0.951619898101\n",
      "4. 0   100   180   0.961674799452\n",
      "3. 0   100   154   0.962517076515\n",
      "2. 0   200   49   0.97291267957\n",
      "1. 0   150   49   0.992010511641\n",
      "Word 27\n",
      "5. 0   350   12   0.979059999676\n",
      "4. 0   100   45   0.981379697209\n",
      "3. 0   400   167   0.981636283821\n",
      "2. 0   400   29   0.982424854533\n",
      "1. 0   400   168   0.988654904123\n",
      "Word 28\n",
      "5. 0   400   169   0.979606720747\n",
      "4. 0   350   177   0.979795436813\n",
      "3. 0   350   200   0.980335495705\n",
      "2. 0   350   189   0.98359630029\n",
      "1. 0   400   189   0.985465392703\n",
      "Word 29\n",
      "5. 0   150   27   0.940850459523\n",
      "4. 0   200   22   0.9562143983\n",
      "3. 0   550   233   0.962256837095\n",
      "2. 0   150   28   0.964769816163\n",
      "1. 0   200   28   0.967080817534\n",
      "Word 30\n",
      "5. 1   100   158   0.971659533919\n",
      "4. 0   200   6   0.971688805755\n",
      "3. 0   150   29   0.972641139846\n",
      "2. 0   0   50   0.978502568538\n",
      "1. 0   150   50   0.983445495106\n",
      "Word 31\n",
      "5. 0   50   80   0.924002518783\n",
      "4. 0   0   61   0.926961680555\n",
      "3. 0   150   18   0.954388315388\n",
      "2. 0   50   79   0.956934454744\n",
      "1. 0   50   190   0.97212700642\n",
      "Word 32\n",
      "5. 0   350   110   0.95744295209\n",
      "4. 0   350   48   0.958111882026\n",
      "3. 0   350   46   0.962593082716\n",
      "2. 0   350   39   0.967098070285\n",
      "1. 0   350   53   0.968521071774\n",
      "Word 33\n",
      "5. 0   450   71   0.940505072308\n",
      "4. 0   450   18   0.944001548927\n",
      "3. 0   150   229   0.96655086415\n",
      "2. 0   250   67   0.967309736514\n",
      "1. 0   150   225   0.9832206274\n",
      "Word 34\n",
      "5. 0   200   237   0.984402504286\n",
      "4. 0   150   237   0.985712204235\n",
      "3. 0   0   42   0.987013233752\n",
      "2. 0   200   239   0.989494050808\n",
      "1. 0   400   21   0.990979164717\n",
      "Word 35\n",
      "5. 0   150   159   0.991314752018\n",
      "4. 0   150   169   0.992012268582\n",
      "3. 0   200   168   0.992608239387\n",
      "2. 0   150   176   0.992660189969\n",
      "1. 0   150   168   0.993235075015\n",
      "Word 36\n",
      "5. 0   200   27   0.962538982661\n",
      "4. 0   350   20   0.963928059641\n",
      "3. 0   200   15   0.975753065513\n",
      "2. 0   200   22   0.976872040429\n",
      "1. 0   400   20   0.98128651099\n",
      "Word 37\n",
      "5. 0   0   238   0.957735428682\n",
      "4. 0   0   234   0.977800888928\n",
      "3. 0   0   239   0.978089291005\n",
      "2. 0   0   42   0.978706993151\n",
      "1. 0   0   50   0.9808310873\n",
      "Word 38\n",
      "5. 0   450   46   0.959086479502\n",
      "4. 0   150   27   0.966468820981\n",
      "3. 0   150   40   0.968592600035\n",
      "2. 0   100   40   0.973254071162\n",
      "1. 0   100   48   0.979739403964\n",
      "Word 39\n",
      "5. 0   200   214   0.97213147546\n",
      "4. 0   450   74   0.973132686801\n",
      "3. 0   450   70   0.976728479653\n",
      "2. 0   200   213   0.97719205957\n",
      "1. 0   300   54   0.978977922116\n",
      "Word 40\n",
      "5. 0   400   120   0.989619632089\n",
      "4. 0   350   35   0.989724790569\n",
      "3. 0   400   114   0.990729064933\n",
      "2. 0   400   111   0.991254873026\n",
      "1. 0   400   77   0.991361172228\n",
      "Word 41\n",
      "5. 0   450   132   0.965007830153\n",
      "4. 0   350   169   0.975329206482\n",
      "3. 0   500   33   0.976739878422\n",
      "2. 0   500   144   0.977704489548\n",
      "1. 0   500   132   0.98216931766\n",
      "Word 42\n",
      "5. 0   250   143   0.964787987238\n",
      "4. 0   200   230   0.969395701407\n",
      "3. 0   150   48   0.981778239139\n",
      "2. 0   150   63   0.985482779855\n",
      "1. 0   550   237   0.986012546915\n",
      "Word 43\n",
      "5. 1   200   124   0.962048600081\n",
      "4. 1   200   145   0.963771510162\n",
      "3. 1   200   116   0.968844482038\n",
      "2. 1   200   123   0.973746703197\n",
      "1. 1   200   135   0.976357035465\n",
      "Word 44\n",
      "5. 0   200   29   0.98960276687\n",
      "4. 0   200   40   0.990493931071\n",
      "3. 0   200   50   0.995372077664\n",
      "2. 0   200   49   0.995872541257\n",
      "1. 0   0   50   0.996565564062\n",
      "Word 45\n",
      "5. 0   100   176   0.976974318306\n",
      "4. 0   350   168   0.98082342956\n",
      "3. 0   150   23   0.983277934657\n",
      "2. 0   150   29   0.985911250395\n",
      "1. 0   150   28   0.988805301542\n",
      "Word 46\n",
      "5. 0   200   15   0.981596255422\n",
      "4. 0   200   21   0.986367689689\n",
      "3. 0   200   5   0.986886153606\n",
      "2. 0   150   41   0.987407630162\n",
      "1. 0   200   41   0.988481730429\n",
      "Word 47\n",
      "5. 0   150   56   0.984156523928\n",
      "4. 0   400   61   0.985746636099\n",
      "3. 0   200   229   0.986546848283\n",
      "2. 0   200   42   0.986647981972\n",
      "1. 0   150   22   0.988192260529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 48\n",
      "5. 0   350   35   0.968103552335\n",
      "4. 0   400   20   0.968583601181\n",
      "3. 0   350   27   0.968891579628\n",
      "2. 0   400   28   0.969389647754\n",
      "1. 0   350   28   0.983240594961\n",
      "Word 49\n",
      "5. 0   200   45   0.98979890738\n",
      "4. 0   350   45   0.990059237737\n",
      "3. 0   200   37   0.992450898855\n",
      "2. 0   150   45   0.995012933186\n",
      "1. 0   150   37   0.996334526224\n",
      "Word 50\n",
      "5. 0   200   48   0.979865184758\n",
      "4. 0   200   49   0.981283562034\n",
      "3. 0   250   49   0.98306766454\n",
      "2. 0   400   16   0.984656503826\n",
      "1. 0   400   41   0.986023813352\n",
      "Word 51\n",
      "5. 0   350   47   0.96071671844\n",
      "4. 0   400   47   0.963664076851\n",
      "3. 0   350   54   0.96423368876\n",
      "2. 0   400   48   0.969842259838\n",
      "1. 0   400   40   0.971734931756\n",
      "Word 52\n",
      "5. 1   100   158   0.919936115518\n",
      "4. 0   200   29   0.929608951417\n",
      "3. 0   200   40   0.93598538443\n",
      "2. 0   150   61   0.943716469402\n",
      "1. 0   400   40   0.96072682107\n",
      "Word 53\n",
      "5. 0   0   206   0.958539334046\n",
      "4. 0   0   196   0.960257650168\n",
      "3. 0   0   183   0.966388761764\n",
      "2. 0   0   195   0.972333079294\n",
      "1. 0   0   185   0.978408305595\n",
      "Word 54\n",
      "5. 0   100   30   0.982435504997\n",
      "4. 0   400   40   0.982553384683\n",
      "3. 0   100   18   0.984683673373\n",
      "2. 0   100   17   0.985876234152\n",
      "1. 0   50   30   0.992491032991\n",
      "Word 55\n",
      "5. 1   450   203   0.858835729892\n",
      "4. 0   450   33   0.871344345403\n",
      "3. 0   400   48   0.909430356895\n",
      "2. 0   400   35   0.915965500533\n",
      "1. 0   400   49   0.969845693696\n",
      "Word 56\n",
      "5. 0   150   238   0.983307859246\n",
      "4. 0   150   216   0.983364914662\n",
      "3. 0   200   237   0.987055482877\n",
      "2. 1   100   152   0.98841807078\n",
      "1. 0   150   49   0.988783612363\n",
      "Word 57\n",
      "5. 0   350   29   0.984188423449\n",
      "4. 0   350   35   0.985650777721\n",
      "3. 0   350   40   0.989778054331\n",
      "2. 0   400   40   0.990826570287\n",
      "1. 0   400   29   0.992606767743\n",
      "Word 58\n",
      "5. 0   200   50   0.988847756196\n",
      "4. 0   200   41   0.991539408299\n",
      "3. 0   200   15   0.996550462881\n",
      "2. 0   200   22   0.996724755145\n",
      "1. 0   200   28   0.997881158552\n",
      "Word 59\n",
      "5. 0   450   35   0.952246099421\n",
      "4. 0   400   17   0.961332410445\n",
      "3. 0   400   244   0.96698247646\n",
      "2. 0   250   45   0.979612411892\n",
      "1. 0   400   24   0.985457276201\n",
      "Word 60\n",
      "5. 0   400   254   0.95920508727\n",
      "4. 0   300   129   0.96910588691\n",
      "3. 0   300   160   0.969238078079\n",
      "2. 0   300   152   0.973296907455\n",
      "1. 0   300   161   0.979916835566\n",
      "Word 61\n",
      "5. 0   150   29   0.974142016372\n",
      "4. 0   150   41   0.980661522737\n",
      "3. 0   150   49   0.98670048725\n",
      "2. 0   100   49   0.986957970982\n",
      "1. 0   150   50   0.991300971243\n",
      "Word 62\n",
      "5. 0   400   135   0.975847137339\n",
      "4. 0   50   142   0.977390016658\n",
      "3. 0   50   178   0.97980783283\n",
      "2. 0   50   171   0.980291211896\n",
      "1. 0   50   80   0.981934833977\n"
     ]
    }
   ],
   "source": [
    "fixedc = int(tWidth/tEx)\n",
    "ptrr = 0\n",
    "for t in range(tStart, tEnd-tWidth+1, tIncr):\n",
    "    ptrppp = 0\n",
    "    for tEStart in range(t,t+tWidth-tEx+1,tEx):\n",
    "        timeSequences[:,:,ptrr,:,:,ptrppp] = np.average(TrainingData[:,:,:,:,tEStart:tEStart+fixedc], axis = 4)\n",
    "        ptrppp+=1\n",
    "    ptrr+=1\n",
    "print(str(timeSequences.shape))\n",
    "\n",
    "for wordnum in range(total_words):\n",
    "    SHheap = [] #heap of BTC + featurevector information used to select top 400\n",
    "    \n",
    "    for band_num in range(5): #frequency bands\n",
    "        ptrr=0\n",
    "        for t in range(tStart, tEnd-tWidth+1, tIncr): #various starts of time slice\n",
    "            for channel in range(256): #eeg channels\n",
    "\n",
    "                #pairwise correlations\n",
    "                avg_p = 0\n",
    "                avg_p2 = 0\n",
    "                #print(str(wordnum) + \" \" + str(band_num) + \" \" + str(ptrr) + \" \" + str(channel))\n",
    "                for i in range(training_amt-1):\n",
    "                    for j in range(i+1,training_amt):\n",
    "                        #if(wordnum == 1):\n",
    "                       #     print(str(pearsonr(timeSequences[wordnum][band_num][ptrr][channel][i],timeSequences[wordnum][band_num][ptrr][channel][j])))\n",
    "                        avg_p += pearsonr(timeSequences[wordnum][band_num][ptrr][i][channel],timeSequences[wordnum][band_num][ptrr][j][channel])[0]\n",
    "\n",
    "                '''\n",
    "                for word2 in range(total_words):\n",
    "                    if(wordnum==word2):\n",
    "                        continue\n",
    "                    avg_p2 += pearsonr(AverageWord[wordnum][band_num][ptrr][channel], AverageWord[word2][band_num][ptrr][channel])[0]\n",
    "                '''\n",
    "                avg_p /= training_amt*(training_amt-1)/2 #want to maximize\n",
    "                #avg_p2 /= (total_words-1) #want to minimize\n",
    "                #ranking_measure = (avg_p - avg_p2)/2 #want to maximize\n",
    "                if(len(SHheap)<400):\n",
    "                    heappush(SHheap, (avg_p,band_num,t,channel, timeSequences[wordnum,band_num,ptrr,:,channel]))\n",
    "                else:\n",
    "                    heappushpop(SHheap, (avg_p,band_num,t,channel, timeSequences[wordnum,band_num,ptrr,:,channel]))\n",
    "            ptrr+=1\n",
    "    #pick top 5\n",
    "    \n",
    "    #f.write(\"Word \" + str(wordnum) +\"\\n\")\n",
    "    print(\"Word \" + str(wordnum))\n",
    "\n",
    "    \n",
    "    current_matrix = np.zeros( (training_amt,0))\n",
    "    test_matrix = np.zeros( (testing_amt,0))\n",
    "    \n",
    "    for i in range(400):\n",
    "        (avg_p,band_num,t,channel, timeSequenc) = heappop(SHheap)\n",
    "        if(i>=400-toSelect):\n",
    "            #this is da guy\n",
    "            #f.write(str(400-i) + \". \" + str(band_num) + \"   \" + str(t) + \"   \" + str(channel) + \"   \" + str(avg_p) + \"\\n\")\n",
    "            print(str(400-i) + \". \" + str(band_num) + \"   \" + str(t) + \"   \" + str(channel) + \"   \" + str(avg_p))\n",
    "            current_matrix = np.hstack( (current_matrix,timeSequenc))\n",
    "\n",
    "            #construct testing matrix\n",
    "            tmpo = np.zeros( (testing_amt,tEx))\n",
    "            for itero in range(testing_amt):\n",
    "                pp = 0\n",
    "                for tEStart in range(t,t+tWidth-tEx+1,tEx):\n",
    "                    tmpo[itero][pp] = np.average(TestingData[wordnum,band_num,itero,channel,tEStart:tEStart+int(tWidth/tEx)])\n",
    "                    pp+=1\n",
    "            test_matrix = np.hstack( (test_matrix,tmpo) )\n",
    "            \n",
    "    best_feature_vectors[wordnum] = current_matrix\n",
    "    test_feature_vectors[wordnum] = test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up for running the actual training and testing between every pair of categories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionaries for storing all data\n",
    "save_trainx = {}\n",
    "save_trainy = {}\n",
    "save_testx = {}\n",
    "save_testy = {}\n",
    "\n",
    "#initialize variable for overall average accuracy\n",
    "avgacc = 0 \n",
    "\n",
    "#initialize space to search for optimal C values\n",
    "clist = np.logspace(-3,2,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and testing (on separate data of course) for every combination of category pairs using a linear kernel SVM. Save and print results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0 and 1 we picked C = 8.69749002618\n",
      "Has accuracy 0.636363636364\n",
      "======\n",
      "For 0 and 2 we picked C = 2.42012826479\n",
      "Has accuracy 0.730769230769\n",
      "======\n",
      "For 0 and 3 we picked C = 35.1119173422\n",
      "Has accuracy 0.636363636364\n",
      "======\n",
      "For 0 and 4 we picked C = 44.3062145758\n",
      "Has accuracy 0.727272727273\n",
      "======\n",
      "For 0 and 5 we picked C = 12.3284673944\n",
      "Has accuracy 0.590909090909\n",
      "======\n",
      "For 0 and 6 we picked C = 0.16681005372\n",
      "Has accuracy 0.590909090909\n",
      "======\n",
      "For 0 and 7 we picked C = 10.9749876549\n",
      "Has accuracy 0.636363636364\n",
      "======\n",
      "For 0 and 8 we picked C = 89.0215085445\n",
      "Has accuracy 0.636363636364\n",
      "======\n",
      "For 0 and 9 we picked C = 8.69749002618\n",
      "Has accuracy 0.636363636364\n",
      "======\n",
      "For 0 and 10 we picked C = 0.533669923121\n",
      "Has accuracy 0.818181818182\n",
      "======\n",
      "For 0 and 11 we picked C = 5.46227721768\n",
      "Has accuracy 0.772727272727\n",
      "======\n",
      "For 1 and 2 we picked C = 0.599484250319\n",
      "Has accuracy 0.5\n",
      "======\n",
      "For 1 and 3 we picked C = 49.7702356433\n",
      "Has accuracy 0.6\n",
      "======\n",
      "For 1 and 4 we picked C = 39.4420605944\n",
      "Has accuracy 0.4\n",
      "======\n",
      "For 1 and 5 we picked C = 15.5567614393\n",
      "Has accuracy 0.75\n",
      "======\n",
      "For 1 and 6 we picked C = 2.42012826479\n",
      "Has accuracy 0.5\n",
      "======\n",
      "For 1 and 7 we picked C = 55.9081018251\n",
      "Has accuracy 0.7\n",
      "======\n",
      "For 1 and 8 we picked C = 17.4752840001\n",
      "Has accuracy 0.6\n",
      "======\n",
      "For 1 and 9 we picked C = 6.89261210435\n",
      "Has accuracy 0.7\n",
      "======\n",
      "For 1 and 10 we picked C = 35.1119173422\n",
      "Has accuracy 0.7\n",
      "======\n",
      "For 1 and 11 we picked C = 3.43046928631\n",
      "Has accuracy 0.7\n",
      "======\n",
      "For 2 and 3 we picked C = 70.5480231072\n",
      "Has accuracy 0.5\n",
      "======\n",
      "For 2 and 4 we picked C = 55.9081018251\n",
      "Has accuracy 0.5\n",
      "======\n",
      "For 2 and 5 we picked C = 44.3062145758\n",
      "Has accuracy 0.666666666667\n",
      "======\n",
      "For 2 and 6 we picked C = 0.16681005372\n",
      "Has accuracy 0.583333333333\n",
      "======\n",
      "For 2 and 7 we picked C = 24.7707635599\n",
      "Has accuracy 0.75\n",
      "======\n",
      "For 2 and 8 we picked C = 5.46227721768\n",
      "Has accuracy 0.666666666667\n",
      "======\n",
      "For 2 and 9 we picked C = 62.8029144183\n",
      "Has accuracy 0.708333333333\n",
      "======\n",
      "For 2 and 10 we picked C = 0.00141747416293\n",
      "Has accuracy 0.75\n",
      "======\n",
      "For 2 and 11 we picked C = 89.0215085445\n",
      "Has accuracy 0.875\n",
      "======\n",
      "For 3 and 4 we picked C = 44.3062145758\n",
      "Has accuracy 0.55\n",
      "======\n",
      "For 3 and 5 we picked C = 3.43046928631\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 3 and 6 we picked C = 0.148496826225\n",
      "Has accuracy 0.6\n",
      "======\n",
      "For 3 and 7 we picked C = 12.3284673944\n",
      "Has accuracy 0.5\n",
      "======\n",
      "For 3 and 8 we picked C = 0.00572236765935\n",
      "Has accuracy 0.5\n",
      "======\n",
      "For 3 and 9 we picked C = 3.43046928631\n",
      "Has accuracy 0.6\n",
      "======\n",
      "For 3 and 10 we picked C = 1.91791026167\n",
      "Has accuracy 0.75\n",
      "======\n",
      "For 3 and 11 we picked C = 12.3284673944\n",
      "Has accuracy 0.8\n",
      "======\n",
      "For 4 and 5 we picked C = 5.46227721768\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 4 and 6 we picked C = 0.148496826225\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 4 and 7 we picked C = 31.2571584969\n",
      "Has accuracy 0.7\n",
      "======\n",
      "For 4 and 8 we picked C = 13.8488637139\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 4 and 9 we picked C = 8.69749002618\n",
      "Has accuracy 0.6\n",
      "======\n",
      "For 4 and 10 we picked C = 3.85352859371\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 4 and 11 we picked C = 39.4420605944\n",
      "Has accuracy 0.85\n",
      "======\n",
      "For 5 and 6 we picked C = 27.8255940221\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 5 and 7 we picked C = 4.32876128108\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 5 and 8 we picked C = 0.756463327555\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 5 and 9 we picked C = 6.89261210435\n",
      "Has accuracy 0.6\n",
      "======\n",
      "For 5 and 10 we picked C = 13.8488637139\n",
      "Has accuracy 0.5\n",
      "======\n",
      "For 5 and 11 we picked C = 1.91791026167\n",
      "Has accuracy 0.55\n",
      "======\n",
      "For 6 and 7 we picked C = 7.74263682681\n",
      "Has accuracy 0.4\n",
      "======\n",
      "For 6 and 8 we picked C = 15.5567614393\n",
      "Has accuracy 0.45\n",
      "======\n",
      "For 6 and 9 we picked C = 12.3284673944\n",
      "Has accuracy 0.7\n",
      "======\n",
      "For 6 and 10 we picked C = 3.85352859371\n",
      "Has accuracy 0.75\n",
      "======\n",
      "For 6 and 11 we picked C = 3.05385550883\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 7 and 8 we picked C = 44.3062145758\n",
      "Has accuracy 0.7\n",
      "======\n",
      "For 7 and 9 we picked C = 24.7707635599\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 7 and 10 we picked C = 49.7702356433\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 7 and 11 we picked C = 1.91791026167\n",
      "Has accuracy 0.85\n",
      "======\n",
      "For 8 and 9 we picked C = 7.74263682681\n",
      "Has accuracy 0.5\n",
      "======\n",
      "For 8 and 10 we picked C = 5.46227721768\n",
      "Has accuracy 0.7\n",
      "======\n",
      "For 8 and 11 we picked C = 0.533669923121\n",
      "Has accuracy 0.45\n",
      "======\n",
      "For 9 and 10 we picked C = 2.42012826479\n",
      "Has accuracy 0.85\n",
      "======\n",
      "For 9 and 11 we picked C = 2.15443469003\n",
      "Has accuracy 0.65\n",
      "======\n",
      "For 10 and 11 we picked C = 70.5480231072\n",
      "Has accuracy 0.75\n",
      "======\n",
      "Average score was 0.65457192983\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "for cat1 in range(12):\n",
    "    for cat2 in range(cat1+1,12):\n",
    "        ptr = 0\n",
    "        tot_words = int(lengths[cat1][0])+int(lengths[cat2][0])\n",
    "        \n",
    "        trainx = np.zeros ( ( training_amt * tot_words,toSelect * tEx))\n",
    "        trainy = np.zeros( (training_amt * tot_words) )\n",
    "        testx = np.zeros ( (testing_amt * tot_words, toSelect*tEx))\n",
    "        testy = np.zeros( (testing_amt * tot_words))\n",
    "        ptr2 = 0\n",
    "        for itero in range(training_amt):\n",
    "            while ptr<7 and category_info[cat1][ptr]!=-1:\n",
    "                tword = category_info[cat1][ptr]\n",
    "                \n",
    "                trainx[ptr2] = best_feature_vectors[tword][itero]\n",
    "                trainy[ptr2] = 0\n",
    "                ptr2+=1\n",
    "                \n",
    "                ptr+=1\n",
    "            ptr = 0\n",
    "            while ptr < 7 and category_info[cat2][ptr]!=-1:\n",
    "                tword = category_info[cat2][ptr]\n",
    "                \n",
    "                trainx[ptr2] = best_feature_vectors[tword][itero]\n",
    "                trainy[ptr2] = 1\n",
    "                ptr2+=1\n",
    "                ptr+=1\n",
    "        ptr = 0\n",
    "        ptr2=0\n",
    "        for itero in range(testing_amt):\n",
    "            while ptr < 7 and category_info[cat1][ptr]!=-1:\n",
    "                tword = category_info[cat1][ptr]\n",
    "                testx[ptr2] = test_feature_vectors[tword][itero]\n",
    "                testy[ptr2] = 0\n",
    "                ptr2+=1\n",
    "                ptr+=1\n",
    "            ptr = 0\n",
    "            while ptr < 7 and category_info[cat2][ptr]!=-1:\n",
    "                tword = category_info[cat2][ptr]\n",
    "\n",
    "                testx[ptr2] = test_feature_vectors[tword][itero]\n",
    "                testy[ptr2] = 1\n",
    "                ptr2+=1\n",
    "                ptr+=1\n",
    "            ptr = 0\n",
    "        save_trainx[(cat1,cat2)] = trainx\n",
    "        save_trainy[(cat1,cat2)] = trainy\n",
    "        save_testx[(cat1,cat2)] = testx\n",
    "        save_testy[(cat1,cat2)] = testy\n",
    "\n",
    "        #run kfold cross validation for parameters\n",
    "        bst_acc = 0\n",
    "        bst_c = 0\n",
    "        \n",
    "        for c in clist:\n",
    "                avg_acc = 0\n",
    "                for fold in range(4):\n",
    "                    fold_sz = int(training_amt*tot_words/4)\n",
    "                    valid_x = trainx[(fold_sz*fold):((fold_sz)*(fold+1))]\n",
    "                    valid_y = trainy[(fold_sz*fold):((fold_sz)*(fold+1))]\n",
    "                    tr_x = np.concatenate( (trainx[0:(fold_sz*fold)],trainx[(fold+1)*fold_sz:(training_amt*tot_words)]), axis = 0)\n",
    "                    tr_y = np.concatenate( (trainy[0:(fold_sz*fold)],trainy[(fold+1)*fold_sz:(training_amt*tot_words)]), axis = 0)\n",
    "\n",
    "                    scaler = StandardScaler()\n",
    "                    tr_x = scaler.fit_transform(tr_x)\n",
    "                    valid_x = scaler.transform(valid_x)\n",
    "                    tr_y = np.ravel(tr_y)\n",
    "                    valid_y = np.ravel(valid_y)\n",
    "\n",
    "                    classifier = LinearSVC(C=c)\n",
    "                    classifier.fit(tr_x,tr_y)\n",
    "                    avg_acc += (classifier.score(valid_x,valid_y) )/4.0\n",
    "                if(avg_acc > bst_acc):\n",
    "                    bst_acc = avg_acc\n",
    "                    bst_c = c\n",
    "                    \n",
    "        print(\"For \" + str(cat1) + \" and \" + str(cat2) + \" we picked C = \" + str(bst_c))\n",
    "        \n",
    "        #create final classifier with best C value\n",
    "        classifier = LinearSVC(C=bst_c)\n",
    "        \n",
    "        #adjust data to the classifier\n",
    "        scaler = StandardScaler()\n",
    "        trainx = scaler.fit_transform(trainx)\n",
    "        testx = scaler.transform(testx)\n",
    "        trainy = np.ravel(trainy)\n",
    "        testy = np.ravel(testy)\n",
    "        \n",
    "        #fit training data to classifier\n",
    "        classifier.fit(trainx, trainy)\n",
    "        #test on testing data\n",
    "        myscore = classifier.score(testx,testy)\n",
    "        avgacc+=myscore\n",
    "        print(\"Has accuracy \" + str(myscore))\n",
    "        print(\"======\")\n",
    "\n",
    "pickle.dump( (save_trainx,save_trainy,save_testx,save_testy), open(\"created_data.p\",\"wb\") )\n",
    "avgacc/=(12*11/2)\n",
    "print(\"Average score was \" + str(avgacc))\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================\n",
    "\n",
    "Final Statistics: \n",
    "\n",
    "Average score:\n",
    "    0.65457192983\n",
    "\n",
    "Best pair score: \n",
    "    0.875 with categories 2 and 11 and C = 89.0215085445\n",
    "\n",
    "========================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
